[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/tmax5.scala.scala","languageId":"scala","version":1,"text":"\nimport org.apache.spark.sql.SparkSession\nimport scalafx.application.JFXApp\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DateType\nimport org.apache.spark.sql.types.DoubleType\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport swiftvis2.plotting._\nimport swiftvis2.plotting.ColorGradient\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\nobject tmax5 extends JFXApp {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n  \n  spark.sparkContext.setLogLevel(\"WARN\")\n  \n  val sschema = StructType(Array(\n      StructField(\"sid\", StringType),\n      StructField(\"lat\", DoubleType),\n      StructField(\"lon\", DoubleType),\n      StructField(\"name\", StringType)\n      ))\n      \n  val stationRDD = spark.sparkContext.textFile(\"D:/Data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).toDouble\n    val lon = line.substring(21, 30).toDouble\n    val name = line.substring(41, 71)\n    Row(id, lat, lon, name)\n  }\n\n  val stations = spark.createDataFrame(stationRDD, sschema).cache()\n  \n  val tschema = StructType(Array(\n      StructField(\"sid\",StringType),\n      StructField(\"date\",DateType),\n      StructField(\"mtype\",StringType),\n      StructField(\"value\",DoubleType)\n      ))\n    \n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2022.csv\").cache()\n  val data2021 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2021.csv\").cache()\n  val data2020 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2020.csv\").cache()\n  val data2019 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2019.csv\").cache()\n  val data2018 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2018.csv\").cache()\n\n\n  val data1972 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1972.csv\").cache()\n  val data1973 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1973.csv\").cache()\n  val data1974 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1974.csv\").cache()\n  val data1975 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1975.csv\").cache()\n  val data1976 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1976.csv\").cache()\n\n  //date1972.show()\n\n  //now\n  val tmin2022 = data2022.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin22\")\n  val tmin2021 = data2021.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin21\")\n  val tmin2020 = data2020.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin20\")\n  val tmin2019 = data2019.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin19\")\n  val tmin2018 = data2018.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin18\")\n  val tmin1972 = data1972.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin72\")\n  val tmin1973 = data1973.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin73\")\n  val tmin1974 = data1974.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin74\")\n  val tmin1975 = data1975.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin75\")\n  val tmin1976 = data1976.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin76\")\n\n  val stationTmin22 = tmin2022.groupBy('sid).agg(avg('tmin22) as \"tmin22\")\n  val stationTmin21 = tmin2021.groupBy('sid).agg(avg('tmin21) as \"tmin21\")\n  val stationTmin20 = tmin2020.groupBy('sid).agg(avg('tmin20) as \"tmin20\")\n  val stationTmin19 = tmin2019.groupBy('sid).agg(avg('tmin19) as \"tmin19\")\n  val stationTmin18 = tmin2018.groupBy('sid).agg(avg('tmin18) as \"tmin18\")\n  val stationTmin72 = tmin1972.groupBy('sid).agg(avg('tmin72) as \"tmin72\")\n  val stationTmin73 = tmin1973.groupBy('sid).agg(avg('tmin73) as \"tmin73\")\n  val stationTmin74 = tmin1974.groupBy('sid).agg(avg('tmin74) as \"tmin74\")\n  val stationTmin75 = tmin1975.groupBy('sid).agg(avg('tmin75) as \"tmin75\")\n  val stationTmin76 = tmin1976.groupBy('sid).agg(avg('tmin76) as \"tmin76\")\n  \n  //tmax\n  val tmax1972 = data1972.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax72\")\n  val tmax1973 = data1973.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax73\")\n  val tmax1974 = data1974.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax74\")\n  val tmax1975 = data1975.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax75\")\n  val tmax1976 = data1976.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax76\")\n  val tmax2022 = data2022.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax22\")\n  val tmax2021 = data2021.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax21\")\n  val tmax2020 = data2020.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax20\")\n  val tmax2019 = data2019.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax19\")\n  val tmax2018 = data2018.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax18\")\n  \n  val stationTmax72 = tmax1972.groupBy('sid).agg(avg('tmax72) as \"tmax72\") \n  val stationTmax73 = tmax1973.groupBy('sid).agg(avg('tmax73) as \"tmax73\")\n  val stationTmax74 = tmax1974.groupBy('sid).agg(avg('tmax74) as \"tmax74\")\n  val stationTmax75 = tmax1975.groupBy('sid).agg(avg('tmax75) as \"tmax75\")\n  val stationTmax76 = tmax1976.groupBy('sid).agg(avg('tmax76) as \"tmax76\")\n  val stationTmax22 = tmax2022.groupBy('sid).agg(avg('tmax22) as \"tmax22\")\n  val stationTmax21 = tmax2021.groupBy('sid).agg(avg('tmax21) as \"tmax21\")\n  val stationTmax20 = tmax2020.groupBy('sid).agg(avg('tmax20) as \"tmax20\")\n  val stationTmax19 = tmax2019.groupBy('sid).agg(avg('tmax19) as \"tmax19\")\n  val stationTmax18 = tmax2018.groupBy('sid).agg(avg('tmax18) as \"tmax18\")\n\n  val joinedminDf = stationTmin22.join(stationTmin21, \"sid\").join(stationTmin20, \"sid\").join(stationTmin19, \"sid\").join(stationTmin18, \"sid\").join(stationTmin72, \"sid\").join(stationTmin73, \"sid\").join(stationTmin74, \"sid\").join(stationTmin75, \"sid\").join(stationTmin76, \"sid\").join(stations, \"sid\")\n  val joinedmaxDf = stationTmax72.join(stationTmax73, \"sid\").join(stationTmax74, \"sid\").join(stationTmax75, \"sid\").join(stationTmax76, \"sid\").join(stationTmax22, \"sid\").join(stationTmax21, \"sid\").join(stationTmax20, \"sid\").join(stationTmax19, \"sid\").join(stationTmax18, \"sid\").join(stations, \"sid\")\n\n  //joinedmaxpastDf.show()\n\n  val combinedTempsmax = joinedmaxDf.select('sid, 'name, 'lat, 'lon, ('tmax22 + 'tmax21 + 'tmax20 + 'tmax19 + 'tmax18 - 'tmax72 - 'tmax73 - 'tmax74 - 'tmax75 - 'tmax76)/50 as \"tavemax\")\n  val combinedTempsmin = joinedminDf.select('sid, 'name, 'lat, 'lon, ('tmin18 + 'tmin19 + 'tmin20 + 'tmin21 + 'tmin22 - 'tmin72 - 'tmin73 - 'tmin74 - 'tmin75 - 'tmin76)/50 as \"tavemin\")\n\n  //val combinedData = combinedTemps1972.join(combinedTemps2022, \"sid\").select('sid, 'name, 'lat, 'lon, ('tave2022 - 'tave1972) as \"vari\")\n  \n  val upstmax = combinedTempsmax.filter(col(\"tavemax\") > 0.0)\n  val lonsupstmax = upstmax.select('lon).as[Double].collect()\n  val latsupstmax = upstmax.select('lat).as[Double].collect()\n  val variupstmax = upstmax.select('tavemax).as[Double].collect()\n\n  val downstmax = combinedTempsmax.filter(col(\"tavemax\") < 0.0)\n  val lonsdownstmax = downstmax.select('lon).as[Double].collect()\n  val latsdownstmax = downstmax.select('lat).as[Double].collect()\n  val varidownstmax = downstmax.select('tavemax).as[Double].collect()\n\n  val upstmin = combinedTempsmin.filter(col(\"tavemin\") > 0.0)\n  val lonsupstmin = upstmin.select('lon).as[Double].collect()\n  val latsupstmin = upstmin.select('lat).as[Double].collect()\n  val variupstmin = upstmin.select('tavemin).as[Double].collect()\n  \n  val downstmin = combinedTempsmin.filter(col(\"tavemin\") < 0.0)\n  val lonsdownstmin = downstmin.select('lon).as[Double].collect()\n  val latsdownstmin = downstmin.select('lat).as[Double].collect()\n  val varidownstmin = downstmin.select('tavemin).as[Double].collect()\n\n  println(\"Tmax increase: \"+variupstmax.length)\n  println(\"Tmax decrease: \"+varidownstmin.length)\n  println(\"Tmin increase: \"+variupstmin.length)\n  println(\"Tmin decrease: \"+varidownstmin.length)\n\n  {\n    val cg = ColorGradient(0.0 -> RedARGB)\n    val plot1 = Plot.scatterPlot(lonsupstmax, latsupstmax, title = \"Stations w/ increase in Max. Temp\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(variupstmax))\n    SwingRenderer(plot1, 800, 600)\n  }\n  {\n    val cg = ColorGradient(0.0 -> BlueARGB)\n    val plot2 = Plot.scatterPlot(lonsdownstmax, latsdownstmax, title = \"Stations w/ decrease in Max. Temp\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(varidownstmax))\n    SwingRenderer(plot2, 800, 600)\n  }\n\n  {\n    val cg = ColorGradient(0.0 -> RedARGB)\n    val plot3 = Plot.scatterPlot(lonsupstmin, latsupstmin, title = \"Stations w/ increase in Min. Temp\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(variupstmin))\n    SwingRenderer(plot3, 800, 600)\n  }\n  {\n    val cg = ColorGradient(0.0 -> BlueARGB)\n    val plot4 = Plot.scatterPlot(lonsdownstmin, latsdownstmin, title = \"Stations w/ decrease in Min. Temp\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(varidownstmin))\n    SwingRenderer(plot4, 800, 600)\n  }\n\nspark.stop()\n}\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Projects\NOAAData\noaadata\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 1 s, completed 31 Mar, 2023 10:30:04 PM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
