[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/trying.scala","languageId":"scala","version":1,"text":"import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DateType\nimport org.apache.spark.sql.types.DoubleType\nimport swiftvis2.plotting\nimport swiftvis2.plotting._\nimport scalafx.application.JFXApp\nimport swiftvis2.plotting.renderer.FXRenderer\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport scala.concurrent.Future\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\nobject trying extends JFXApp {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n\n  spark.sparkContext.setLogLevel(\"WARN\")\n\n  val stations = spark.read.textFile(\"src/main/scala/Data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).trim.toDouble\n    val lon = line.substring(21, 30).trim.toDouble\n    val elev = line.substring(31, 37).trim.toDouble\n    val name = line.substring(41, 71)\n    Station(id, lat, lon, elev, name)\n  }.cache()\n\n  val tschema = StructType(Array(\n      StructField(\"sid\",StringType),\n      StructField(\"date\",DateType),\n      StructField(\"mtype\",StringType),\n      StructField(\"value\",DoubleType)\n      ))\n\n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/data/1972.csv\").cache()\n\n  val tmax2022 = data2022.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax\")\n  val dailyTemp2022 = tmax2022.select('sid, 'date, ('tmax)/10 as \"tmx2022\")\n  val stationTemp2022 = dailyTemp2022.groupBy('sid).agg(avg('tmx2022) as \"tmx2022\")\n  val joinedData2022 = stationTemp2022.join(stations, \"sid\")\n\n\n  val stationsVA = new VectorAssembler().setInputCols(Array(\"tmx2022\")).setOutputCol(\"Max Temp\")\n  val stationsWithLoc = stationsVA.transform(joinedData2022)\n  //  stationsWithLoc.show()\n\n  val kMeans = new KMeans().setK(10).setFeaturesCol(\"Max Temp\").setPredictionCol(\"cluster\")\n  val stationClusterModel = kMeans.fit(stationsWithLoc)\n\n  val stationsWithClusters = stationClusterModel.transform(stationsWithLoc)\n  stationsWithClusters.show()\n\n  //  println(kMeans.explainParams())\n  val clu = stationsWithClusters.select('cluster).as[Int].collect()\n  val lons = stationsWithClusters.select('lon).as[Double].collect()\n  val lats = stationsWithClusters.select('lat).as[Double].collect()\n\n  //count of stations in each cluster\n  stationsWithClusters.groupBy('cluster).count().show()\n\n  {\n    implicit val df = stationsWithClusters\n   \t\tval cg = ColorGradient(0.0 -> BlueARGB, 10.0 -> RedARGB, 5.0 -> GreenARGB)\n   \t\tval plot = Plot.scatterPlot(lons, lats, title = \"Stations clustered into 10 zones based on Max Temp(1972)\", xLabel = \"Longitude\", yLabel = \"Latitude\",\n  \t\tsymbolSize = 3, symbolColor = cg(clu))\n  \t\tSwingRenderer(plot, 1000, 650)\n  }\n\n\n  spark.stop()\n}\n\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Projects\NOAAData\noaadata\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 2 s, completed 17 Feb, 2023 9:45:40 PM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, â™¨1, shutdown, null})[0m
