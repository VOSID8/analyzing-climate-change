[0m[[0m[0mdebug[0m] [0m[0m> Exec(collectAnalyses, None, Some(CommandSource(network-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Processing event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/tmax5.scala.scala","languageId":"scala","version":1,"text":"\nimport org.apache.spark.sql.SparkSession\nimport scalafx.application.JFXApp\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DateType\nimport org.apache.spark.sql.types.DoubleType\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport swiftvis2.plotting._\nimport swiftvis2.plotting.ColorGradient\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\nobject tmax5 extends JFXApp {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n  \n  spark.sparkContext.setLogLevel(\"WARN\")\n  \n  val sschema = StructType(Array(\n      StructField(\"sid\", StringType),\n      StructField(\"lat\", DoubleType),\n      StructField(\"lon\", DoubleType),\n      StructField(\"name\", StringType)\n      ))\n      \n  val stationRDD = spark.sparkContext.textFile(\"D:/Data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).toDouble\n    val lon = line.substring(21, 30).toDouble\n    val name = line.substring(41, 71)\n    Row(id, lat, lon, name)\n  }\n\n  val stations = spark.createDataFrame(stationRDD, sschema).cache()\n  \n  val tschema = StructType(Array(\n      StructField(\"sid\",StringType),\n      StructField(\"date\",DateType),\n      StructField(\"mtype\",StringType),\n      StructField(\"value\",DoubleType)\n      ))\n    \n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2022.csv\").cache()\n  val data2021 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2021.csv\").cache()\n  val data2020 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2020.csv\").cache()\n  val data2019 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2019.csv\").cache()\n  val data2018 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2018.csv\").cache()\n\n\n  val data1972 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1972.csv\").cache()\n  val data1973 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1973.csv\").cache()\n  val data1974 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1974.csv\").cache()\n  val data1975 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1975.csv\").cache()\n  val data1976 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1976.csv\").cache()\n\n  //date1972.show()\n\n  //now\n  val tmin2022 = data2022.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin22\")\n  val tmin2021 = data2021.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin21\")\n  val tmin2020 = data2020.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin20\")\n  val tmin2019 = data2019.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin19\")\n  val tmin2018 = data2018.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin18\")\n  val tmin1972 = data1972.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin72\")\n  val tmin1973 = data1973.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin73\")\n  val tmin1974 = data1974.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin74\")\n  val tmin1975 = data1975.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin75\")\n  val tmin1976 = data1976.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin76\")\n\n  val stationTmin22 = tmin2022.groupBy('sid).agg(avg('tmin22) as \"tmin22\")\n  val stationTmin21 = tmin2021.groupBy('sid).agg(avg('tmin21) as \"tmin21\")\n  val stationTmin20 = tmin2020.groupBy('sid).agg(avg('tmin20) as \"tmin20\")\n  val stationTmin19 = tmin2019.groupBy('sid).agg(avg('tmin19) as \"tmin19\")\n  val stationTmin18 = tmin2018.groupBy('sid).agg(avg('tmin18) as \"tmin18\")\n  val stationTmin72 = tmin1972.groupBy('sid).agg(avg('tmin72) as \"tmin72\")\n  val stationTmin73 = tmin1973.groupBy('sid).agg(avg('tmin73) as \"tmin73\")\n  val stationTmin74 = tmin1974.groupBy('sid).agg(avg('tmin74) as \"tmin74\")\n  val stationTmin75 = tmin1975.groupBy('sid).agg(avg('tmin75) as \"tmin75\")\n  val stationTmin76 = tmin1976.groupBy('sid).agg(avg('tmin76) as \"tmin76\")\n  \n  //tmax\n  val tmax1972 = data1972.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax72\")\n  val tmax1973 = data1973.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax73\")\n  val tmax1974 = data1974.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax74\")\n  val tmax1975 = data1975.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax75\")\n  val tmax1976 = data1976.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax76\")\n  val tmax2022 = data2022.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax22\")\n  val tmax2021 = data2021.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax21\")\n  val tmax2020 = data2020.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax20\")\n  val tmax2019 = data2019.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax19\")\n  val tmax2018 = data2018.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax18\")\n  \n  val stationTmax72 = tmax1972.groupBy('sid).agg(avg('tmax72) as \"tmax72\") \n  val stationTmax73 = tmax1973.groupBy('sid).agg(avg('tmax73) as \"tmax73\")\n  val stationTmax74 = tmax1974.groupBy('sid).agg(avg('tmax74) as \"tmax74\")\n  val stationTmax75 = tmax1975.groupBy('sid).agg(avg('tmax75) as \"tmax75\")\n  val stationTmax76 = tmax1976.groupBy('sid).agg(avg('tmax76) as \"tmax76\")\n  val stationTmax22 = tmax2022.groupBy('sid).agg(avg('tmax22) as \"tmax22\")\n  val stationTmax21 = tmax2021.groupBy('sid).agg(avg('tmax21) as \"tmax21\")\n  val stationTmax20 = tmax2020.groupBy('sid).agg(avg('tmax20) as \"tmax20\")\n  val stationTmax19 = tmax2019.groupBy('sid).agg(avg('tmax19) as \"tmax19\")\n  val stationTmax18 = tmax2018.groupBy('sid).agg(avg('tmax18) as \"tmax18\")\n\n  val joinedminDf = stationTmin22.join(stationTmin21, \"sid\").join(stationTmin20, \"sid\").join(stationTmin19, \"sid\").join(stationTmin18, \"sid\").join(stationTmin72, \"sid\").join(stationTmin73, \"sid\").join(stationTmin74, \"sid\").join(stationTmin75, \"sid\").join(stationTmin76, \"sid\").join(stations, \"sid\")\n  val joinedmaxDf = stationTmax72.join(stationTmax73, \"sid\").join(stationTmax74, \"sid\").join(stationTmax75, \"sid\").join(stationTmax76, \"sid\").join(stationTmax22, \"sid\").join(stationTmax21, \"sid\").join(stationTmax20, \"sid\").join(stationTmax19, \"sid\").join(stationTmax18, \"sid\").join(stations, \"sid\")\n\n  //joinedmaxpastDf.show()\n\n  val combinedTempsmax = joinedmaxDf.select('sid, 'name, 'lat, 'lon, ('tmax22 + 'tmax21 + 'tmax20 + 'tmax19 + 'tmax18 - 'tmax72 - 'tmax73 - 'tmax74 - 'tmax75 - 'tmax76)/50 as \"tavemax\")\n  val combinedTempsmin = joinedminDf.select('sid, 'name, 'lat, 'lon, ('tmin18 + 'tmin19 + 'tmin20 + 'tmin21 + 'tmin22 - 'tmin72 - 'tmin73 - 'tmin74 - 'tmin75 - 'tmin76)/50 as \"tavemin\")\n\n  //val combinedData = combinedTemps1972.join(combinedTemps2022, \"sid\").select('sid, 'name, 'lat, 'lon, ('tave2022 - 'tave1972) as \"vari\")\n  \n  val upstmax = combinedTempsmax.filter(col(\"tavemax\") > 0.0)\n  val lonsupstmax = upstmax.select('lon).as[Double].collect()\n  val latsupstmax = upstmax.select('lat).as[Double].collect()\n  val variupstmax = upstmax.select('tavemax).as[Double].collect()\n\n  val downstmax = combinedTempsmax.filter(col(\"tavemax\") < 0.0)\n  val lonsdownstmax = downstmax.select('lon).as[Double].collect()\n  val latsdownstmax = downstmax.select('lat).as[Double].collect()\n  val varidownstmax = downstmax.select('tavemax).as[Double].collect()\n\n  val upstmin = combinedTempsmin.filter(col(\"tavemin\") > 0.0)\n  val lonsupstmin = upstmin.select('lon).as[Double].collect()\n  val latsupstmin = upstmin.select('lat).as[Double].collect()\n  val variupstmin = upstmin.select('tavemin).as[Double].collect()\n  \n  val downstmin = combinedTempsmin.filter(col(\"tavemin\") < 0.0)\n  val lonsdownstmin = downstmin.select('lon).as[Double].collect()\n  val latsdownstmin = downstmin.select('lat).as[Double].collect()\n  val varidownstmin = downstmin.select('tavemin).as[Double].collect()\n\n  println(\"Tmax increase: \"+variupstmax.length)\n  println(\"Tmax decrease: \"+varidownstmin.length)\n  println(\"Tmin increase: \"+variupstmin.length)\n  println(\"Tmin decrease: \"+varidownstmin.length)\n\n  {\n    val cg = ColorGradient(0.0 -> RedARGB)\n    val plot1 = Plot.scatterPlot(lonsupstmax, latsupstmax, title = \"Stations w/ increase in Max. Temp\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(variupstmax))\n    SwingRenderer(plot1, 800, 600)\n  }\n  {\n    val cg = ColorGradient(0.0 -> BlueARGB)\n    val plot2 = Plot.scatterPlot(lonsdownstmax, latsdownstmax, title = \"Stations w/ decrease in Max. Temp\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(varidownstmax))\n    SwingRenderer(plot2, 800, 600)\n  }\n\n  {\n    val cg = ColorGradient(0.0 -> RedARGB)\n    val plot3 = Plot.scatterPlot(lonsupstmin, latsupstmin, title = \"Stations w/ increase in Min. Temp\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(variupstmin))\n    SwingRenderer(plot3, 800, 600)\n  }\n  {\n    val cg = ColorGradient(0.0 -> BlueARGB)\n    val plot4 = Plot.scatterPlot(lonsdownstmin, latsdownstmin, title = \"Stations w/ decrease in Min. Temp\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(varidownstmin))\n    SwingRenderer(plot4, 800, 600)\n  }\n\nspark.stop()\n}\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / collectAnalyses[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0manalysis location (C:\Projects\NOAAData\noaadata\target\scala-2.12\zinc\inc_compile_2.12.zip,true)[0m
[0m[[0m[32msuccess[0m] [0m[0mTotal time: 1 s, completed 31 Mar, 2023 10:36:42 PM[0m
[0m[[0m[0mdebug[0m] [0m[0munmatched Done event for requestId None: None[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/Ditemp2022.scala","languageId":"scala","version":1,"text":"\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didClose: JsonRpcNotificationMessage(2.0, textDocument/didClose, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/Ditemp2022.scala"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/Ditempcommon.scala","languageId":"scala","version":1,"text":"\r\nimport org.apache.spark.sql.SparkSession\r\nimport scalafx.application.JFXApp\r\nimport org.apache.spark.sql.types.StructType\r\nimport org.apache.spark.sql.types.StructField\r\nimport org.apache.spark.sql.types.StringType\r\nimport org.apache.spark.sql.types.DateType\r\nimport org.apache.spark.sql.types.DoubleType\r\nimport org.apache.spark.sql.Row\r\nimport org.apache.spark.sql.functions._\r\nimport swiftvis2.plotting._\r\nimport swiftvis2.plotting.ColorGradient\r\nimport swiftvis2.plotting.renderer.SwingRenderer\r\n\r\n\r\nobject NOAAData extends JFXApp {\r\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\r\n  import spark.implicits._\r\n  \r\n  spark.sparkContext.setLogLevel(\"WARN\")\r\n  \r\n  val tschema = StructType(Array(\r\n      StructField(\"sid\",StringType),\r\n      StructField(\"date\",DateType),\r\n      StructField(\"mtype\",StringType),\r\n      StructField(\"value\",DoubleType)\r\n      ))\r\n    \r\n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/data/2022.csv\").cache()\r\n  val data1952 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/data/1952.csv\").cache()\r\n  \r\n  val sschema = StructType(Array(\r\n      StructField(\"sid\", StringType),\r\n      StructField(\"lat\", DoubleType),\r\n      StructField(\"lon\", DoubleType),\r\n      StructField(\"name\", StringType)\r\n      ))\r\n  val stationRDD = spark.sparkContext.textFile(\"src/main/scala/data/ghcnd-stations.txt\").map { line =>\r\n    val id = line.substring(0, 11)\r\n    val lat = line.substring(12, 20).toDouble\r\n    val lon = line.substring(21, 30).toDouble\r\n    val name = line.substring(41, 71)\r\n    Row(id, lat, lon, name)\r\n  }\r\n  val stations = spark.createDataFrame(stationRDD, sschema).cache()\r\n  \r\n  val tmax2022 = data2022.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax\")\r\n  val tmin2022 = data2022.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin\")\r\n  val combinedTemps2022 = tmax2022.join(tmin2022, Seq(\"sid\", \"date\"))\r\n  val dailyTemp2022 = combinedTemps2022.select('sid, 'date, ('tmax - 'tmin)/10 as \"tave2022\")\r\n  val stationTemp2022 = dailyTemp2022.groupBy('sid).agg(avg('tave2022) as \"tave2022\")\r\n  val joinedData2022 = stationTemp2022.join(stations, \"sid\")\r\n\r\n  val tmax1952 = data1952.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax\")\r\n  val tmin1952 = data1952.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin\")\r\n  val combinedTemps1952 = tmax1952.join(tmin1952, Seq(\"sid\", \"date\"))\r\n  val dailyTemp1952 = combinedTemps1952.select('sid, 'date, ('tmax - 'tmin)/10 as \"tave1952\")\r\n  val stationTemp1952 = dailyTemp1952.groupBy('sid).agg(avg('tave1952) as \"tave1952\")\r\n\r\n  val combinedData = stationTemp1952.join(joinedData2022,\"sid\")\r\n\r\n  combinedData.show()\r\n  combinedData.schema.printTreeString()\r\n  \r\n  \r\n  val filteredData = combinedData.select('lon, 'lat, 'tave1952,'tave2022).as[(Double, Double, Double,Double)].collect()\r\n  //val redpoints = joinedData2022.filter('tave >= 15.0).select('lon, 'lat, 'tave).as[(Double, Double, Double)].collect()\r\n  //val bluepoints = joinedData2022.filter('tave <= 5.0).select('lon, 'lat, 'tave).as[(Double, Double, Double)].collect()\r\n  //val greenpoints = joinedData2022.filter('tave > 5.0 && 'tave < 15).select('lon, 'lat, 'tave).as[(Double, Double, Double)].collect()\r\n  val sizeredpoints1952 = filteredData.count(_._3 > 15.0)\r\n  val sizebluepoints1952 = filteredData.count(_._3 <= 5.0)\r\n  val sizegreenpoints1952 = filteredData.count(_._3 > 5.0) - sizeredpoints1952\r\n\r\n  val sizeredpoints2022 = filteredData.count(_._4 > 15.0)\r\n  val sizebluepoints2022 = filteredData.count(_._4 <= 5.0)\r\n  val sizegreenpoints2022 = filteredData.count(_._4 > 5.0) - sizeredpoints2022\r\n  println(sizeredpoints1952)\r\n  println(sizebluepoints1952)\r\n  println(sizegreenpoints1952)\r\n  println(sizeredpoints2022)\r\n  println(sizebluepoints2022)\r\n  println(sizegreenpoints2022)\r\n\r\n  // val lons = joinedData2022.select('lon).as[Double].collect()\r\n  // val lats = joinedData2022.select('lat).as[Double].collect()\r\n  // val taves = joinedData2022.select('tave).as[Double].collect()\r\n\r\n  // {\r\n    // val cg = ColorGradient(5.0 -> BlueARGB, 10.0 -> GreenARGB, 15.0 -> RedARGB)\r\n    // val plot = Plot.scatterPlot(lons, lats, title = \"Difference Temps 1952\", xLabel = \"Longitude\", \r\n        // yLabel = \"Latitude\", symbolSize = 3, cg(taves))\r\n    // SwingRenderer(plot, 600, 500)\r\n  // }\r\n\r\nspark.stop()\r\n}\r\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/tmax.scala","languageId":"scala","version":1,"text":"\nimport org.apache.spark.sql.SparkSession\nimport scalafx.application.JFXApp\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DateType\nimport org.apache.spark.sql.types.DoubleType\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport swiftvis2.plotting._\nimport swiftvis2.plotting.ColorGradient\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\nobject tmax extends JFXApp {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n  \n  spark.sparkContext.setLogLevel(\"WARN\")\n  \n  val sschema = StructType(Array(\n      StructField(\"sid\", StringType),\n      StructField(\"lat\", DoubleType),\n      StructField(\"lon\", DoubleType),\n      StructField(\"name\", StringType)\n      ))\n      \n  val stationRDD = spark.sparkContext.textFile(\"D:/Data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).toDouble\n    val lon = line.substring(21, 30).toDouble\n    val name = line.substring(41, 71)\n    Row(id, lat, lon, name)\n  }\n\n  val stations = spark.createDataFrame(stationRDD, sschema).cache()\n  \n  val tschema = StructType(Array(\n      StructField(\"sid\",StringType),\n      StructField(\"date\",DateType),\n      StructField(\"mtype\",StringType),\n      StructField(\"value\",DoubleType)\n      ))\n    \n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2022.csv\").cache()\n  val data2021 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2021.csv\").cache()\n  val data2020 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2020.csv\").cache()\n  val data2019 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2019.csv\").cache()\n  val data2018 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2018.csv\").cache()\n\n\n  val data1972 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1972.csv\").cache()\n  val data1973 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1973.csv\").cache()\n  val data1974 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1974.csv\").cache()\n  val data1975 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1975.csv\").cache()\n  val data1976 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1976.csv\").cache()\n\n  //date1972.show()\n\n  //now\n  val tmin2022 = data2022.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin22\")\n  val tmin2021 = data2021.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin21\")\n  val tmin2020 = data2020.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin20\")\n  val tmin2019 = data2019.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin19\")\n  val tmin2018 = data2018.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin18\")\n  val tmin1972 = data1972.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin72\")\n  val tmin1973 = data1973.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin73\")\n  val tmin1974 = data1974.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin74\")\n  val tmin1975 = data1975.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin75\")\n  val tmin1976 = data1976.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin76\")\n\n  val stationTmin22 = tmin2022.groupBy('sid).agg(avg('tmin22) as \"tmin22\")\n  val stationTmin21 = tmin2021.groupBy('sid).agg(avg('tmin21) as \"tmin21\")\n  val stationTmin20 = tmin2020.groupBy('sid).agg(avg('tmin20) as \"tmin20\")\n  val stationTmin19 = tmin2019.groupBy('sid).agg(avg('tmin19) as \"tmin19\")\n  val stationTmin18 = tmin2018.groupBy('sid).agg(avg('tmin18) as \"tmin18\")\n  val stationTmin72 = tmin1972.groupBy('sid).agg(avg('tmin72) as \"tmin72\")\n  val stationTmin73 = tmin1973.groupBy('sid).agg(avg('tmin73) as \"tmin73\")\n  val stationTmin74 = tmin1974.groupBy('sid).agg(avg('tmin74) as \"tmin74\")\n  val stationTmin75 = tmin1975.groupBy('sid).agg(avg('tmin75) as \"tmin75\")\n  val stationTmin76 = tmin1976.groupBy('sid).agg(avg('tmin76) as \"tmin76\")\n  \n  //tmax\n  val tmax1972 = data1972.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax72\")\n  val tmax1973 = data1973.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax73\")\n  val tmax1974 = data1974.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax74\")\n  val tmax1975 = data1975.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax75\")\n  val tmax1976 = data1976.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax76\")\n  val tmax2022 = data2022.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax22\")\n  val tmax2021 = data2021.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax21\")\n  val tmax2020 = data2020.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax20\")\n  val tmax2019 = data2019.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax19\")\n  val tmax2018 = data2018.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax18\")\n  \n  val stationTmax72 = tmax1972.groupBy('sid).agg(avg('tmax72) as \"tmax72\") \n  val stationTmax73 = tmax1973.groupBy('sid).agg(avg('tmax73) as \"tmax73\")\n  val stationTmax74 = tmax1974.groupBy('sid).agg(avg('tmax74) as \"tmax74\")\n  val stationTmax75 = tmax1975.groupBy('sid).agg(avg('tmax75) as \"tmax75\")\n  val stationTmax76 = tmax1976.groupBy('sid).agg(avg('tmax76) as \"tmax76\")\n  val stationTmax22 = tmax2022.groupBy('sid).agg(avg('tmax22) as \"tmax22\")\n  val stationTmax21 = tmax2021.groupBy('sid).agg(avg('tmax21) as \"tmax21\")\n  val stationTmax20 = tmax2020.groupBy('sid).agg(avg('tmax20) as \"tmax20\")\n  val stationTmax19 = tmax2019.groupBy('sid).agg(avg('tmax19) as \"tmax19\")\n  val stationTmax18 = tmax2018.groupBy('sid).agg(avg('tmax18) as \"tmax18\")\n\n  val joinedminDf = stationTmin22.join(stationTmin21, \"sid\").join(stationTmin20, \"sid\").join(stationTmin19, \"sid\").join(stationTmin18, \"sid\").join(stationTmin72, \"sid\").join(stationTmin73, \"sid\").join(stationTmin74, \"sid\").join(stationTmin75, \"sid\").join(stationTmin76, \"sid\").join(stations, \"sid\")\n  val joinedmaxDf = stationTmax72.join(stationTmax73, \"sid\").join(stationTmax74, \"sid\").join(stationTmax75, \"sid\").join(stationTmax76, \"sid\").join(stationTmax22, \"sid\").join(stationTmax21, \"sid\").join(stationTmax20, \"sid\").join(stationTmax19, \"sid\").join(stationTmax18, \"sid\").join(stations, \"sid\")\n\n  //joinedmaxpastDf.show()\n\n  val combinedData = joinedmaxDf.select('sid, 'name, 'lat, 'lon, ('tmax22 + 'tmax21 + 'tmax20 + 'tmax19 + 'tmax18)/50 as \"tmx2022\", ('tmax72 + 'tmax73 + 'tmax74 + 'tmax75 + 'tmax76)/50 as \"tmx1972\").cache()\n  // val combinedTemps1972 = tmax1972.join(tmin1972, Seq(\"sid\", \"date\"))\n  // val dailyTemp1972 = combinedTemps1972.select('sid, 'date, ('tmax)/10 as \"tmx1972\")\n  // val stationTemp1972 = dailyTemp1972.groupBy('sid).agg(avg('tmx1972) as \"tmx1972\")\n\n  // val combinedData = stationTemp1972.join(joinedData2022,\"sid\").withColumn(\"diff\", 'tmx2022 - 'tmx1972)\n\n  println(\"Below -4: \" + combinedData.filter(col(\"tmx1972\") < -4.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") < -4.0).count())\n  println(\"Between -4 & -2: \" + combinedData.filter(col(\"tmx1972\") > -4.0 && col(\"tmx1972\") < -2.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > -4.0 && col(\"tmx2022\") < -2.0).count())\n  println(\"Between -2 & 0: \" + combinedData.filter(col(\"tmx1972\") > -2.0 && col(\"tmx1972\") < 0.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > -2.0 && col(\"tmx2022\") < 0.0).count())\n  println(\"Between 0 & 2: \" + combinedData.filter(col(\"tmx1972\") > 0.0 && col(\"tmx1972\") < 2.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 0.0 && col(\"tmx2022\") < 2.0).count())\n  println(\"Between 2 & 4: \" + combinedData.filter(col(\"tmx1972\") > 2.0 && col(\"tmx1972\") < 4.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 2.0 && col(\"tmx2022\") < 4.0).count())\n  println(\"Between 4 & 6: \" + combinedData.filter(col(\"tmx1972\") > 4.0 && col(\"tmx1972\") < 6.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 4.0 && col(\"tmx2022\") < 6.0).count())\n  println(\"Between 6 & 8: \" + combinedData.filter(col(\"tmx1972\") > 6.0 && col(\"tmx1972\") < 8.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 6.0 && col(\"tmx2022\") < 8.0).count())\n  println(\"Between 8 & 10: \" + combinedData.filter(col(\"tmx1972\") > 8.0 && col(\"tmx1972\") < 10.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 8.0 && col(\"tmx2022\") < 10.0).count())\n  println(\"Between 10 & 12: \" + combinedData.filter(col(\"tmx1972\") > 10.0 && col(\"tmx1972\") < 12.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 10.0 && col(\"tmx2022\") < 12.0).count())\n  println(\"Between 12 & 14: \" + combinedData.filter(col(\"tmx1972\") > 12.0 && col(\"tmx1972\") < 14.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 12.0 && col(\"tmx2022\") < 14.0).count())\n  println(\"Between 14 & 16: \" + combinedData.filter(col(\"tmx1972\") > 14.0 && col(\"tmx1972\") < 16.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 14.0 && col(\"tmx2022\") < 16.0).count())\n  println(\"Between 16 & 18: \" + combinedData.filter(col(\"tmx1972\") > 16.0 && col(\"tmx1972\") < 18.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 16.0 && col(\"tmx2022\") < 18.0).count())\n  println(\"Between 18 & 20: \" + combinedData.filter(col(\"tmx1972\") > 18.0 && col(\"tmx1972\") < 20.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 18.0 && col(\"tmx2022\") < 20.0).count())\n  println(\"Between 20 & 22: \" + combinedData.filter(col(\"tmx1972\") > 20.0 && col(\"tmx1972\") < 22.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 20.0 && col(\"tmx2022\") < 22.0).count())\n  println(\"Between 22 & 24: \" + combinedData.filter(col(\"tmx1972\") > 22.0 && col(\"tmx1972\") < 24.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 22.0 && col(\"tmx2022\") < 24.0).count())\n  println(\"Between 24 & 26: \" + combinedData.filter(col(\"tmx1972\") > 24.0 && col(\"tmx1972\") < 26.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 24.0 && col(\"tmx2022\") < 26.0).count())\n  println(\"Between 26 & 28: \" + combinedData.filter(col(\"tmx1972\") > 26.0 && col(\"tmx1972\") < 28.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 26.0 && col(\"tmx2022\") < 28.0).count())\n  println(\"Between 28 & 30: \" + combinedData.filter(col(\"tmx1972\") > 28.0 && col(\"tmx1972\") < 30.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 28.0 && col(\"tmx2022\") < 30.0).count())\n  println(\"Between 30 & 32: \" + combinedData.filter(col(\"tmx1972\") > 30.0 && col(\"tmx1972\") < 32.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 30.0 && col(\"tmx2022\") < 32.0).count())\n  println(\"Between 32 & 34: \" + combinedData.filter(col(\"tmx1972\") > 32.0 && col(\"tmx1972\") < 34.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 32.0 && col(\"tmx2022\") < 34.0).count())\n  println(\"Between 34 & 36: \" + combinedData.filter(col(\"tmx1972\") > 34.0 && col(\"tmx1972\") < 36.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 34.0 && col(\"tmx2022\") < 36.0).count())\n  println(\"Between 36 & 38: \" + combinedData.filter(col(\"tmx1972\") > 36.0 && col(\"tmx1972\") < 38.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 36.0 && col(\"tmx2022\") < 38.0).count())\n  println(\"Between 38 & 40: \" + combinedData.filter(col(\"tmx1972\") > 38.0 && col(\"tmx1972\") < 40.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 38.0 && col(\"tmx2022\") < 40.0).count())\n\n\n\n\n  //println(\"Between 40 & 42: \" + combinedData.filter(col(\"tmx1972\") > 40.0 && col(\"tmx1972\") < 42.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 40.0 && col(\"tmx2022\") < 42.0).count())\n  //println(\"Between 42 & 44: \" + combinedData.filter(col(\"tmx1972\") > 42.0 && col(\"tmx1972\") < 44.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 42.0 && col(\"tmx2022\") < 44.0).count())\n  //println(\"Between 44 & 46: \" + combinedData.filter(col(\"tmx1972\") > 44.0 && col(\"tmx1972\") < 46.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 44.0 && col(\"tmx2022\") < 46.0).count())\n  //println(\"Above 46: \" + combinedData.filter(col(\"tmx1972\") > 46.0).count() + \"--->\" + combinedData.filter(col(\"tmx2022\") > 46.0).count())\n\n\nspark.stop()\n}\n\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mForcing garbage collection...[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/trying.scala","languageId":"scala","version":1,"text":"import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DateType\nimport org.apache.spark.sql.types.DoubleType\nimport swiftvis2.plotting\nimport swiftvis2.plotting._\nimport scalafx.application.JFXApp\nimport swiftvis2.plotting.renderer.FXRenderer\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport scala.concurrent.Future\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\nobject trying extends JFXApp {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n\n  spark.sparkContext.setLogLevel(\"WARN\")\n\n  val stations = spark.read.textFile(\"src/main/scala/Data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).trim.toDouble\n    val lon = line.substring(21, 30).trim.toDouble\n    val elev = line.substring(31, 37).trim.toDouble\n    val name = line.substring(41, 71)\n    Station(id, lat, lon, elev, name)\n  }.cache()\n\n  val tschema = StructType(Array(\n      StructField(\"sid\",StringType),\n      StructField(\"date\",DateType),\n      StructField(\"mtype\",StringType),\n      StructField(\"value\",DoubleType)\n      ))\n\n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/data/1972.csv\").cache()\n\n  val tmax2022 = data2022.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax\")\n  val dailyTemp2022 = tmax2022.select('sid, 'date, ('tmax)/10 as \"tmx2022\")\n  val stationTemp2022 = dailyTemp2022.groupBy('sid).agg(avg('tmx2022) as \"tmx2022\")\n  val joinedData2022 = stationTemp2022.join(stations, \"sid\")\n\n\n  val stationsVA = new VectorAssembler().setInputCols(Array(\"tmx2022\")).setOutputCol(\"Max Temp\")\n  val stationsWithLoc = stationsVA.transform(joinedData2022)\n  //  stationsWithLoc.show()\n\n  val kMeans = new KMeans().setK(10).setFeaturesCol(\"Max Temp\").setPredictionCol(\"cluster\")\n  val stationClusterModel = kMeans.fit(stationsWithLoc)\n\n  val stationsWithClusters = stationClusterModel.transform(stationsWithLoc)\n  stationsWithClusters.show()\n\n  //  println(kMeans.explainParams())\n  val clu = stationsWithClusters.select('cluster).as[Int].collect()\n  val lons = stationsWithClusters.select('lon).as[Double].collect()\n  val lats = stationsWithClusters.select('lat).as[Double].collect()\n\n  //count of stations in each cluster\n  stationsWithClusters.groupBy('cluster).count().show()\n\n  {\n    implicit val df = stationsWithClusters\n   \t\tval cg = ColorGradient(0.0 -> BlueARGB, 10.0 -> RedARGB, 5.0 -> GreenARGB)\n   \t\tval plot = Plot.scatterPlot(lons, lats, title = \"Stations clustered into 10 zones based on Max Temp(1972)\", xLabel = \"Longitude\", yLabel = \"Latitude\",\n  \t\tsymbolSize = 3, symbolColor = cg(clu))\n  \t\tSwingRenderer(plot, 1000, 650)\n  }\n\n\n  spark.stop()\n}\n\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didClose: JsonRpcNotificationMessage(2.0, textDocument/didClose, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/Ditempcommon.scala"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/tminsize.scala","languageId":"scala","version":1,"text":"\nimport org.apache.spark.sql.SparkSession\nimport scalafx.application.JFXApp\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DateType\nimport org.apache.spark.sql.types.DoubleType\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport swiftvis2.plotting._\nimport swiftvis2.plotting.ColorGradient\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\nobject tminsize extends JFXApp {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n  spark.sparkContext.setLogLevel(\"WARN\")\n  \n  val tschema = StructType(Array(\n      StructField(\"sid\",StringType),\n      StructField(\"date\",DateType),\n      StructField(\"mtype\",StringType),\n      StructField(\"value\",DoubleType)\n      ))\n    \n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/data/2022.csv\").cache()\n  val data1972 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/data/1972.csv\").cache()\n  \n  val sschema = StructType(Array(\n      StructField(\"sid\", StringType),\n      StructField(\"lat\", DoubleType),\n      StructField(\"lon\", DoubleType),\n      StructField(\"name\", StringType)\n      ))\n  val stationRDD = spark.sparkContext.textFile(\"src/main/scala/data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).toDouble\n    val lon = line.substring(21, 30).toDouble\n    val name = line.substring(41, 71)\n    Row(id, lat, lon, name)\n  }\n  val stations = spark.createDataFrame(stationRDD, sschema).cache()\n  \n  val tmax2022 = data2022.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax\")\n  val tmin2022 = data2022.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin\")\n  val combinedTemps2022 = tmax2022.join(tmin2022, Seq(\"sid\", \"date\"))\n  val dailyTemp2022 = combinedTemps2022.select('sid, 'date, ('tmin)/10 as \"tmi2022\")\n  val stationTemp2022 = dailyTemp2022.groupBy('sid).agg(avg('tmi2022) as \"tmi2022\")\n  val joinedData2022 = stationTemp2022.join(stations, \"sid\")\n\n  val tmax1972 = data1972.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax\")\n  val tmin1972 = data1972.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin\")\n  val combinedTemps1972 = tmax1972.join(tmin1972, Seq(\"sid\", \"date\"))\n  val dailyTemp1972 = combinedTemps1972.select('sid, 'date, ('tmin)/10 as \"tmi1972\")\n  val stationTemp1972 = dailyTemp1972.groupBy('sid).agg(avg('tmi1972) as \"tmi1972\")\n\n  val combinedData = stationTemp1972.join(joinedData2022,\"sid\").withColumn(\"diff\", 'tmi2022 - 'tmi1972)\n  \n  //val lons = combinedData.select('lon).as[Double].collect()\n  //val lats = combinedData.select('lat).as[Double].collect()\n  \n  val ups = combinedData.filter(col(\"diff\") > 0.0)\n  val lonsups = ups.select('lon).as[Double].collect()\n  val latsups = ups.select('lat).as[Double].collect()\n  val variups = ups.select('diff).as[Double].collect()\n\n  val downs = combinedData.filter(col(\"diff\") < 0.0)\n  val lonsdowns = downs.select('lon).as[Double].collect()\n  val latsdowns = downs.select('lat).as[Double].collect()\n  val varidowns = downs.select('diff).as[Double].collect()\n\n  println(variups.length)\n  println(varidowns.length)\n\n\nspark.stop()\n}\n\n\n\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/predict.scala","languageId":"scala","version":1,"text":"import org.apache.spark.sql.SparkSession\nimport swiftvis2.plotting\nimport swiftvis2.plotting._\nimport scalafx.application.JFXApp\nimport swiftvis2.plotting.renderer.FXRenderer\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport scala.concurrent.Future\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\n/*\n * NOAA data from ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/  in the by_year directory\n */\n\ncase class Station(sid: String, lat: Double, lon: Double, elev: Double, name: String)\ncase class NOAADataa(sid: String, date: java.sql.Date, measure: String, value: Double)\ncase class ClusterData(num: Int, lat: Double, lon: Double, latstd: Double, lonstd: Double,\n  tmax: Double, tmin: Double, tmaxstd: Double, tminstd: Double, precip: Double,\n  tmaxSeasonalVar: Double, tminSeasonalVar: Double)\n\nobject NOAAClustering extends JFXApp {\n  //  Future {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n\n  spark.sparkContext.setLogLevel(\"WARN\")\n\n  val stations = spark.read.textFile(\"src/main/scala/Data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).trim.toDouble\n    val lon = line.substring(21, 30).trim.toDouble\n    val elev = line.substring(31, 37).trim.toDouble\n    val name = line.substring(41, 71)\n    Station(id, lat, lon, elev, name)\n  }.cache()\n\n  val stationsVA = new VectorAssembler().setInputCols(Array(\"lat\", \"lon\")).setOutputCol(\"location\")\n  val stationsWithLoc = stationsVA.transform(stations)\n  //  stationsWithLoc.show()\n\n  val kMeans = new KMeans().setK(2000).setFeaturesCol(\"location\").setPredictionCol(\"cluster\")\n  val stationClusterModel = kMeans.fit(stationsWithLoc)\n\n  val stationsWithClusters = stationClusterModel.transform(stationsWithLoc)\n  stationsWithClusters.show()\n\n  //  println(kMeans.explainParams())\n  val clu = stationsWithClusters.select('cluster).as[Int].collect()\n  val lons = stationsWithClusters.select('lon).as[Double].collect()\n  val lats = stationsWithClusters.select('lat).as[Double].collect()\n\n  //{\n  //  implicit val df = stationsWithClusters\n  // \t\tval cg = ColorGradient(0.0 -> BlueARGB, 1000.0 -> RedARGB, 2000.0 -> GreenARGB)\n  // \t\tval plot = Plot.scatterPlot(lons, lats, title = \"Stations\", xLabel = \"Longitude\", yLabel = \"Latitude\",\n  //\t\tsymbolSize = 3, symbolColor = cg(clu))\n  //\t\tSwingRenderer(plot, 1000, 650)\n  //}\n\n  val data2017 = spark.read.schema(Encoders.product[NOAADataa].schema).\n    option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/Data/2022.csv\")\n\n  val joinedData = data2017.filter('measure === \"TMAX\").join(stationsWithClusters, \"sid\").cache()\n\n    val withDOYinfo = joinedData.withColumn(\"doy\", dayofyear('date)).\n      withColumn(\"doySin\", sin('doy / 365 * 2 * math.Pi)).\n      withColumn(\"doyCos\", cos('doy / 365 * 2 * math.Pi))\n    val linearRegData = new VectorAssembler().setInputCols(Array(\"doySin\", \"doyCos\")).\n      setOutputCol(\"doyTrig\").transform(withDOYinfo).cache()\n    val linearReg = new LinearRegression().setFeaturesCol(\"doyTrig\").setLabelCol(\"value\").\n      setMaxIter(10).setPredictionCol(\"pmaxTemp\")\n    val linearRegModel = linearReg.fit(linearRegData)\n    val withLinearFit = linearRegModel.transform(linearRegData)\n\n  \n  \n  //Code for plotting the results of the linear regression to fit the sinusoid.\n  //y = a*sin(doy) + b*cos(doy) + c\n  //    val doy = withLinearFit.select('doy).as[Double].collect(): PlotDoubleSeries\n  //    val maxTemp = withLinearFit.select('value).as[Double].collect(): PlotDoubleSeries\n  //    val pmaxTemp = withLinearFit.select('pmaxTemp).as[Double].collect(): PlotDoubleSeries\n  //    val size1 = 3: PlotDoubleSeries\n  //    val size2 = 0: PlotDoubleSeries\n  //    val color = BlackARGB: PlotIntSeries\n  //    val stroke = renderer.Renderer.StrokeData(1, Nil)\n  //    val tempPlot = Plot.scatterPlotsFull(\n  //      Array(\n  //     (doy, maxTemp, color, size1, , None, None),\n  //     (doy, pmaxTemp, color, size2, Some((0: PlotIntSeries) -> stroke), None, None)),\n  //      title = \"High Temps\", xLabel = \"Day of Year\", yLabel = \"Temp\")\n  //    SwingRenderer(tempPlot, 600, 600)\n\n  spark.stop()\n}\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didClose: JsonRpcNotificationMessage(2.0, textDocument/didClose, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/tminsize.scala"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/Ditemp2022.scala","languageId":"scala","version":1,"text":"\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/Ditemp1972.scala","languageId":"scala","version":1,"text":"\nimport org.apache.spark.sql.SparkSession\nimport scalafx.application.JFXApp\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DateType\nimport org.apache.spark.sql.types.DoubleType\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport swiftvis2.plotting._\nimport swiftvis2.plotting.ColorGradient\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\nobject Ditemp1972 extends JFXApp {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n  \n  spark.sparkContext.setLogLevel(\"WARN\")\n\n  val sschema = StructType(Array(\n      StructField(\"sid\", StringType),\n      StructField(\"lat\", DoubleType),\n      StructField(\"lon\", DoubleType),\n      StructField(\"name\", StringType)\n      ))\n      \n  val stationRDD = spark.sparkContext.textFile(\"D:/Data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).toDouble\n    val lon = line.substring(21, 30).toDouble\n    val name = line.substring(41, 71)\n    Row(id, lat, lon, name)\n  }\n\n  val stations = spark.createDataFrame(stationRDD, sschema).cache()\n  \n  val tschema = StructType(Array(\n      StructField(\"sid\",StringType),\n      StructField(\"date\",DateType),\n      StructField(\"mtype\",StringType),\n      StructField(\"value\",DoubleType)\n      ))\n    \n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2022.csv\").cache()\n  val data2021 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2021.csv\").cache()\n  val data2020 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2020.csv\").cache()\n  val data2019 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2019.csv\").cache()\n  val data2018 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/present/2018.csv\").cache()\n\n\n  val data1972 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1972.csv\").cache()\n  val data1973 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1973.csv\").cache()\n  val data1974 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1974.csv\").cache()\n  val data1975 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1975.csv\").cache()\n  val data1976 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"D:/Data/past/1976.csv\").cache()\n\n  //date1972.show()\n\n  //now\n  val tmin2022 = data2022.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin22\")\n  val tmin2021 = data2021.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin21\")\n  val tmin2020 = data2020.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin20\")\n  val tmin2019 = data2019.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin19\")\n  val tmin2018 = data2018.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin18\")\n  val tmin1972 = data1972.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin72\")\n  val tmin1973 = data1973.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin73\")\n  val tmin1974 = data1974.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin74\")\n  val tmin1975 = data1975.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin75\")\n  val tmin1976 = data1976.filter('mtype === \"TMIN\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmin76\")\n\n  val stationTmin22 = tmin2022.groupBy('sid).agg(avg('tmin22) as \"tmin22\")\n  val stationTmin21 = tmin2021.groupBy('sid).agg(avg('tmin21) as \"tmin21\")\n  val stationTmin20 = tmin2020.groupBy('sid).agg(avg('tmin20) as \"tmin20\")\n  val stationTmin19 = tmin2019.groupBy('sid).agg(avg('tmin19) as \"tmin19\")\n  val stationTmin18 = tmin2018.groupBy('sid).agg(avg('tmin18) as \"tmin18\")\n  val stationTmin72 = tmin1972.groupBy('sid).agg(avg('tmin72) as \"tmin72\")\n  val stationTmin73 = tmin1973.groupBy('sid).agg(avg('tmin73) as \"tmin73\")\n  val stationTmin74 = tmin1974.groupBy('sid).agg(avg('tmin74) as \"tmin74\")\n  val stationTmin75 = tmin1975.groupBy('sid).agg(avg('tmin75) as \"tmin75\")\n  val stationTmin76 = tmin1976.groupBy('sid).agg(avg('tmin76) as \"tmin76\")\n\n  // println(\"\\n\")\n  // tmin1972.schema.printTreeString()\n  // println(\"\\n\")\n  // stationTmin72.schema.printTreeString()\n  // println(\"\\n\")\n  \n  //tmax\n  val tmax1972 = data1972.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax72\")\n  val tmax1973 = data1973.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax73\")\n  val tmax1974 = data1974.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax74\")\n  val tmax1975 = data1975.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax75\")\n  val tmax1976 = data1976.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax76\")\n  val tmax2022 = data2022.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax22\")\n  val tmax2021 = data2021.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax21\")\n  val tmax2020 = data2020.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax20\")\n  val tmax2019 = data2019.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax19\")\n  val tmax2018 = data2018.filter($\"mtype\" === \"TMAX\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"tmax18\")\n  \n  val stationTmax72 = tmax1972.groupBy('sid).agg(avg('tmax72) as \"tmax72\") \n  val stationTmax73 = tmax1973.groupBy('sid).agg(avg('tmax73) as \"tmax73\")\n  val stationTmax74 = tmax1974.groupBy('sid).agg(avg('tmax74) as \"tmax74\")\n  val stationTmax75 = tmax1975.groupBy('sid).agg(avg('tmax75) as \"tmax75\")\n  val stationTmax76 = tmax1976.groupBy('sid).agg(avg('tmax76) as \"tmax76\")\n  val stationTmax22 = tmax2022.groupBy('sid).agg(avg('tmax22) as \"tmax22\")\n  val stationTmax21 = tmax2021.groupBy('sid).agg(avg('tmax21) as \"tmax21\")\n  val stationTmax20 = tmax2020.groupBy('sid).agg(avg('tmax20) as \"tmax20\")\n  val stationTmax19 = tmax2019.groupBy('sid).agg(avg('tmax19) as \"tmax19\")\n  val stationTmax18 = tmax2018.groupBy('sid).agg(avg('tmax18) as \"tmax18\")\n\n  val joinedpastDf = stationTmin72.join(stationTmin73, \"sid\").join(stationTmin74, \"sid\").join(stationTmin75, \"sid\").join(stationTmin76, \"sid\").join(stationTmax72, \"sid\").join(stationTmax73, \"sid\").join(stationTmax74, \"sid\").join(stationTmax75, \"sid\").join(stationTmax76, \"sid\").join(stations, \"sid\")\n  val joinednowDf = stationTmin22.join(stationTmin21, \"sid\").join(stationTmin20, \"sid\").join(stationTmin19, \"sid\").join(stationTmin18, \"sid\").join(stationTmax22, \"sid\").join(stationTmax21, \"sid\").join(stationTmax20, \"sid\").join(stationTmax19, \"sid\").join(stationTmax18, \"sid\").join(stations, \"sid\")\n\n  // val joinedmaxpastDf = stationTmax72.join(stationTmax73, \"sid\").join(stationTmax74, \"sid\").join(stationTmax75, \"sid\").join(stationTmax76, \"sid\").join(stations, \"sid\").join(date1972,\"sid\")\n  // val joinedminnowDf = stationTmin22.join(stationTmin21, \"sid\").join(stationTmin20, \"sid\").join(stationTmin19, \"sid\").join(stationTmin18, \"sid\").join(stations, \"sid\").join(date2022,\"sid\")\n  // val joinedmaxnowDf = stationTmax22.join(stationTmax21, \"sid\").join(stationTmax20, \"sid\").join(stationTmax19, \"sid\").join(stationTmax18, \"sid\").join(stations, \"sid\").join(date2022,\"sid\")\n  // val joinedminpastDf = stationTmin72.join(stationTmin73, \"sid\").join(stationTmin74, \"sid\").join(stationTmin75, \"sid\").join(stationTmin76, \"sid\").join(stations, \"sid\").join(date1972,\"sid\")\n\n  //joinedmaxpastDf.show()\n\n  val combinedTemps2022 = joinednowDf.select('sid, 'name, 'lat, 'lon, ('tmax22 + 'tmax21 + 'tmax20 + 'tmax19 + 'tmax18 - 'tmin22 - 'tmin21 - 'tmin20 - 'tmin19 - 'tmin18)/50 as \"tave2022\")\n  val combinedTemps1972 = joinedpastDf.select('sid, ('tmax72 + 'tmax73 + 'tmax74 + 'tmax75 + 'tmax76 - 'tmin72 - 'tmin73 - 'tmin74 - 'tmin75 - 'tmin76)/50 as \"tave1972\")\n\n  // combinedTemps1972.show()\n  // combinedTemps2022.show()\n\n  //tmaxDf1972.show()\n\n  // tmaxDf1972.show()\n\n  ////// val combinedDf1 = joinedInnerDf.join(tmax1973, \"sid\")\n  ////// println(\"Combined  \"+combinedDf1.count().toString()+\"\\n\")\n\n  ////// val combinedDf2 = combinedDf.join(tmax1974, \"sid\")\n  ////// val combinedDf3 = combinedDf2.join(tmax1975, \"sid\")\n  ////// val combinedDf4 = combinedDf3.join(tmax1976, \"sid\")\n  ////// val combinedDf5 = combinedDf4.select('sid, 'tmax72, 'tmax73, 'tmax74, 'tmax75, 'tmax76)\n  \n  ////// val dfList = List(tmax1972, tmax1973, tmax1974, tmax1975, tmax1976)\n  ////// combine all the dataframes such that each \n\n  ////// val tmaxcombpast = tmax1972.union(tmax1973).union(tmax1974).union(tmax1975).union(tmax1976)\n  ////// write code for sum of tmax for each station\n  ////// val tmaxsum = combinedDf.groupBy('sid).agg(sum('tmax).alias(\"tmaxsumm\"))\n  \n  ////// combinedDf.show()\n  ////// val tmaxs5 = tmaxsum.withColumn(\"tmaxsumby5\", tmaxsum(\"tmaxsumm\")/5)\n  ////// tmaxs5.show()\n  ////// val tmaxcomb2past = tmaxs5.join(stations, \"sid\")\n  ////// val tmaxcomb4past = tmaxcomb2past.select('sid, 'name, 'lat, 'lon, 'tmaxsumby5)\n\n  ////// val tmaxcomb2past = tmaxcombpast.groupBy('sid).agg(avg('tmax) as \"maxtemp\")\n  ////// val tmaxcomb3past = tmaxcomb2past.join(stations, \"sid\")\n  ////// val tmaxcomb4past = tmaxcomb3past.select('sid, 'name, 'lat, 'lon, 'maxtemp)\n  ////// tmaxcomb4past.show()\n\n  ////// val tmincombpast = tmin1972.union(tmin1973).union(tmin1974).union(tmin1975).union(tmin1976)\n  ////// val tmincomb2past = tmincombpast.groupBy('sid).agg(avg('tmin) as \"tmin\")\n  ////// val tmincomb3past = tmincomb2past.join(stations, \"sid\")\n  ////// val tmincomb4past = tmincomb3past.select('sid, 'name, 'lat, 'lon, 'tmin)\n\n  //transformations\n\n  ////// val combinedTemps2022 = tmaxDf2022.join(tminDf2022, Seq(\"sid\", \"date\"))\n  // val dailyTemp2022 = combinedTemps2022.select('sid, ('tmax - 'tmin)/10 as \"tave2022\")\n  // val stationTemp2022 = dailyTemp2022.groupBy('sid).agg(avg('tave2022) as \"tave2022\")\n  // val joinedData2022 = stationTemp2022.join(stations, \"sid\")\n\n  ////// val combinedTemps1972 = tmaxDf1972.join(tminDf1972, Seq(\"sid\", \"date\"))\n  // val dailyTemp1972 = combinedTemps1972.select('sid, ('tmax - 'tmin)/10 as \"tave1972\")\n  // val stationTemp1972 = dailyTemp1972.groupBy('sid).agg(avg('tave1972) as \"tave1972\")\n\n  val combinedData = combinedTemps1972.join(combinedTemps2022, \"sid\")\n\n  // combinedData.show()\n  // combinedData.schema.printTreeString()\n  val filteredData = combinedData.select('lon, 'lat, 'tave1972,'tave2022).as[(Double, Double, Double,Double)].collect()\n\n  //nah\n  //val redpoints = joinedData2022.filter('tave >= 17.0).select('lon, 'lat, 'tave).as[(Double, Double, Double)].collect()\n  //val bluepoints = joinedData2022.filter('tave <= 7.0).select('lon, 'lat, 'tave).as[(Double, Double, Double)].collect()\n  //val greenpoints = joinedData2022.filter('tave > 7.0 && 'tave < 17).select('lon, 'lat, 'tave).as[(Double, Double, Double)].collect()\n  //nah\n  \n  \n  val sizeredpoints1972 = filteredData.count(_._3 > 14.0)\n  val sizebluepoints1972 = filteredData.count(_._3 <= 8.0)\n  val sizegreenpoints1972 = filteredData.count(_._3 > 8.0) - sizeredpoints1972\n\n  val sizeredpoints2022 = filteredData.count(_._4 > 14.0)\n  val sizebluepoints2022 = filteredData.count(_._4 <= 8.0)\n  val sizegreenpoints2022 = filteredData.count(_._4 > 8.0) - sizeredpoints2022\n\n\n  println(sizeredpoints1972)\n  println(sizebluepoints1972)\n  println(sizegreenpoints1972)\n\n  println(sizeredpoints2022)\n  println(sizebluepoints2022)\n  println(sizegreenpoints2022)\n\n  // val lons = combinedData.select('lon).as[Double].collect()\n  // val lats = combinedData.select('lat).as[Double].collect()\n  // val taves1972 = combinedData.select('tave1972).as[Double].collect()\n  // val taves2022 = combinedData.select('tave2022).as[Double].collect()\n\n  // {\n  //    val cg = ColorGradient(5.0 -> BlueARGB, 10.0 -> GreenARGB, 15.0 -> RedARGB)\n  //    val plot = Plot.scatterPlot(lons, lats, title = \"Diurnal Temps 1972-76\", xLabel = \"Longitude\", \n  //    yLabel = \"Latitude\", symbolSize = 3, cg(taves1972))\n  //    SwingRenderer(plot, 800, 600)\n  // }\n  // {\n  //    val cg = ColorGradient(5.0 -> BlueARGB, 10.0 -> GreenARGB, 15.0 -> RedARGB)\n  //    val plot = Plot.scatterPlot(lons, lats, title = \"Diurnal Temps 2018-22\", xLabel = \"Longitude\", \n  //    yLabel = \"Latitude\", symbolSize = 3, cg(taves2022))\n  //    SwingRenderer(plot, 800, 600)\n  // }\n\nspark.stop()\n}\n\n"}})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled request received: shutdown: JsonRpcRequestMessage(2.0, ♨1, shutdown, null})[0m
