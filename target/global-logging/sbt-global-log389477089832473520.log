[0m[[0m[0mdebug[0m] [0m[0m> Exec(run, Some(9b2a6c2d-5996-4b3a-bbc7-e5fbc98678fe), Some(CommandSource(console0)))[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / run[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: initialized: JsonRpcNotificationMessage(2.0, initialized, {})[0m
[0m[[0m[0mdebug[0m] [0m[0mUnhandled notification received: textDocument/didOpen: JsonRpcNotificationMessage(2.0, textDocument/didOpen, {"textDocument":{"uri":"file:///c%3A/Projects/NOAAData/noaadata/src/main/scala/please.scala","languageId":"scala","version":1,"text":"\nimport org.apache.spark.sql.SparkSession\nimport scalafx.application.JFXApp\nimport org.apache.spark.sql.types.StructType\nimport org.apache.spark.sql.types.StructField\nimport org.apache.spark.sql.types.StringType\nimport org.apache.spark.sql.types.DateType\nimport org.apache.spark.sql.types.DoubleType\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.functions._\nimport swiftvis2.plotting._\nimport swiftvis2.plotting.ColorGradient\nimport swiftvis2.plotting.renderer.SwingRenderer\n\n\nobject please extends JFXApp {\n  val spark = SparkSession.builder().master(\"local[*]\").appName(\"NOAA Data\").getOrCreate()\n  import spark.implicits._\n  \n  spark.sparkContext.setLogLevel(\"WARN\")\n  \n  val tschema = StructType(Array(\n      StructField(\"sid\",StringType),\n      StructField(\"date\",DateType),\n      StructField(\"mtype\",StringType),\n      StructField(\"value\",DoubleType)\n      ))\n    \n  val data2022 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/data/2022.csv\").cache()\n  val data1972 = spark.read.schema(tschema).option(\"dateFormat\", \"yyyyMMdd\").csv(\"src/main/scala/data/1972.csv\").cache()\n  \n  val sschema = StructType(Array(\n      StructField(\"sid\", StringType),\n      StructField(\"lat\", DoubleType),\n      StructField(\"lon\", DoubleType),\n      StructField(\"name\", StringType)\n      ))\n  val stationRDD = spark.sparkContext.textFile(\"src/main/scala/data/ghcnd-stations.txt\").map { line =>\n    val id = line.substring(0, 11)\n    val lat = line.substring(12, 20).toDouble\n    val lon = line.substring(21, 30).toDouble\n    val name = line.substring(41, 71)\n    Row(id, lat, lon, name)\n  }\n  val stations = spark.createDataFrame(stationRDD, sschema).cache()\n  \n  val prcp2022 = data2022.filter($\"mtype\" === \"PRCP\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"prcp\")\n  val dailyPrp2022 = prcp2022.select('sid, 'date, ('prcp)/10 as \"pri2022\")\n  val stationPrp2022 = dailyPrp2022.groupBy('sid).agg(avg('pri2022) as \"pri2022\")\n  val joinedData2022 = stationPrp2022.join(stations, \"sid\")\n\n  val prcp1972 = data1972.filter($\"mtype\" === \"PRCP\").limit(10000000).drop(\"mtype\").withColumnRenamed(\"value\", \"prcp\")\n  val dailyPrp1972 = prcp1972.select('sid, 'date, ('prcp)/10 as \"pri1972\")\n  val stationPrp1972 = dailyPrp1972.groupBy('sid).agg(avg('pri1972) as \"pri1972\")\n  \n  val combinedData = stationPrp1972.join(joinedData2022,\"sid\")\n  \n  //val lons = combinedData.select('lon).as[Double].collect()\n  //val lats = combinedData.select('lat).as[Double].collect()\n\n  val downs = combinedData.filter(col(\"pri1972\") < 250.0)\n  val lonsdowns = downs.select('lon).as[Double].collect()\n  val latsdowns = downs.select('lat).as[Double].collect()\n  val varidowns = downs.select('pri1972).as[Double].collect()\n\n  //val ups = combinedData.filter(col(\"pri2022\") < 500.0)\n  //val lonsups = ups.select('lon).as[Double].collect()\n  //val latsups = ups.select('lat).as[Double].collect()\n  //val variups = ups.select('pri2022).as[Double].collect()\n\n  println(varidowns.length)\n  //println(variups.length)\n  \n  \n  //{\n  //  val cg = ColorGradient(0.0 -> RedARGB)\n  //  val plot1 = Plot.scatterPlot(lonsups, latsups, title = \"Semi-Arid stations in 2022\", xLabel = \"Longitude\", \n  //      yLabel = \"Latitude\", symbolSize = 3, cg(variups))\n  //  SwingRenderer(plot1, 900, 700)\n  //}\n  {\n    val cg = ColorGradient(0.0 -> BlueARGB)\n    val plot2 = Plot.scatterPlot(lonsdowns, latsdowns, title = \"Semi-Arid stations in 1972\", xLabel = \"Longitude\", \n        yLabel = \"Latitude\", symbolSize = 3, cg(varidowns))\n    SwingRenderer(plot2, 900, 700)\n  }\n\nspark.stop()\n}\n\n\n\n\n\n\n"}})[0m
[0m[[0m[31merror[0m] [0m[0mjava.lang.RuntimeException: Exception in Application start method[0m
[0m[[0m[31merror[0m] [0m[0m	at com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:917)[0m
[0m[[0m[31merror[0m] [0m[0m	at com.sun.javafx.application.LauncherImpl.lambda$launchApplication$1(LauncherImpl.java:182)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:750)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 1.0 failed 1 times, most recent failure: Lost task 10.0 in stage 1.0 (TID 22) (LAPTOP-1OB7IE1N executor driver): java.io.IOException: There is not enough space on the disk[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.IOUtil.write(IOUtil.java:51)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:447)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedDataWithChannel(BypassMergeSortShuffleWriter.java:246)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:218)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.Task.run(Task.scala:136)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:750)[0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0mDriver stacktrace:[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.foreach(Option.scala:274)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[0m
[0m[[0m[31merror[0m] [0m[0mCaused by: java.io.IOException: There is not enough space on the disk[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.IOUtil.write(IOUtil.java:51)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)[0m
[0m[[0m[31merror[0m] [0m[0m	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:447)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedDataWithChannel(BypassMergeSortShuffleWriter.java:246)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:218)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:180)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.scheduler.Task.run(Task.scala:136)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.lang.Thread.run(Thread.java:750)[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":1,"message":"java.lang.RuntimeException: Exception in Application start method\r\n\tat com.sun.javafx.application.LauncherImpl.launchApplication1(LauncherImpl.java:917)\r\n\tat com.sun.javafx.application.LauncherImpl.lambda$launchApplication$1(LauncherImpl.java:182)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 1.0 failed 1 times, most recent failure: Lost task 10.0 in stage 1.0 (TID 22) (LAPTOP-1OB7IE1N executor driver): java.io.IOException: There is not enough space on the disk\r\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\r\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:75)\r\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\r\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\r\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\r\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\r\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)\r\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:447)\r\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedDataWithChannel(BypassMergeSortShuffleWriter.java:246)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:218)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:180)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:274)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.IOException: There is not enough space on the disk\r\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\r\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:75)\r\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\r\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\r\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\r\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\r\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)\r\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:447)\r\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedDataWithChannel(BypassMergeSortShuffleWriter.java:246)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:218)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:180)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)"})[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) Exception in Application start method[0m
[0m[[0m[0mdebug[0m] [0m[0mjsonRpcNotify: JsonRpcNotificationMessage(2.0, build/logMessage, {"type":1,"message":"(Compile / \u001b[31mrun\u001b[0m) Exception in Application start method"})[0m
[0m[[0m[31merror[0m] [0m[0mTotal time: 106 s (01:46), completed 13 Feb, 2023 1:11:30 PM[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(shell, None, None)[0m
